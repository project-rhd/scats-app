#! /bin/sh

#======= Primary Settings Start ==============================

# Ip address of spark master.
spark_master_address=scats-1-master
# Ip address of hadoop master. (the name node)
hadoop_master_address=scats-1-master
# Directory in hdfs for placing jar. e.g. /scats
jar_hdfs_dir=/scats

# Local Spark home directory e.g. /usr/local/spark
client_spark_home=
# Local Hadoop home e.g. /usrl/local/hadoop
client_hadoop_home=

accumulo_instance_id=
# scats-1-master:2181,scats-1-slave:2181,scats-2-slave:2181
zookeepers=
accumulo_user_name=
accumulo_user_pwd=
accumulo_table_name=

# Path of raw data file in hdfs. e.g. /scats/sample700M.csv  VolumeData.CSV  VolumeData.sample.csv
INPUT_VOLUME_FILE=/scats/VolumeData.sample.csv
INPUT_LAYOUT_FILE=/scats/SiteLayouts.csv
INPUT_SHAPE_FILE=/scats/scats_site_vic_4326.csv


#======= Primary Settings End ================================

app_base=$(dirname $(readlink -f $0))/../data-importer
hdfs_root=hdfs://${hadoop_master_address}:9000
input_volume_url=${hdfs_root}${INPUT_VOLUME_FILE}
input_layouts_url=${hdfs_root}${INPUT_LAYOUT_FILE}
input_shp_url=${hdfs_root}${INPUT_SHAPE_FILE}

echo "Task 1: Build application jar locally"
mvn -f ${app_base}/../pom.xml -pl data-importer clean package -am
if [ $? -eq 0 ]; then
    echo "Task 1 Finished"
else
    echo "Error. Failed in task 1"
    exit 1
fi

echo "Task 2: Upload jar to remote HDFS:" ${hdfs_root}${jar_hdfs_dir}
jar_name=$(ls ${app_base}/target/ | grep scats-.*.jar)
${client_hadoop_home}/bin/hdfs dfs -put -f ${app_base}/target/${jar_name} ${hdfs_root}${jar_hdfs_dir}
if [ $? -eq 0 ]; then
    echo "Task 2 Finished"
else
    echo "Error. Failed in Task 2"
    exit 1
fi


# Option: Use "--conf spark.hadoop.validateOutputSpecs=false" for overwriting existing files.
# But better to delete the whole output directory if exists before saving.
echo "Task 3: Submit task in cluster mode to the spark master:" spark://${spark_master_address}:6066
${client_spark_home}/bin/spark-submit \
--master spark://${spark_master_address}:6066 \
--driver-memory 3G \
--executor-memory 2G \
--class "scats.sparkApp.DataImporter" \
--deploy-mode cluster \
${hdfs_root}${jar_hdfs_dir}/${jar_name} \
--instanceId ${accumulo_instance_id} \
--zookeepers ${zookeepers} \
--user ${accumulo_user_name} \
--password ${accumulo_user_pwd} \
--tableName ${accumulo_table_name} \
--inputVolumeFile ${input_volume_url} \
--inputLayoutFile ${input_layouts_url} \
--inputShapeFile ${input_shp_url}

if [ $? -eq 0 ]; then
    echo "Task 3 Finished"
else
    echo "Error. Failed in Task 3"
    exit 1
fi

echo "Task submitted. Check" http://${spark_master_address}:8080 "for details."
